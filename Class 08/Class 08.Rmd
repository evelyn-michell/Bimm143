---
title: "Machine Learning 1"
author: "Evelyn Diaz - PID:A15576617"
date: "10/21/2021"
output: html_document
---

First up is clusterking methods
# Kmeans clustering

The function in base R to do Kmeans clustering is called `kmeans()`. 

First make up some data where we know what the answer should be:

```{r}
tmp<- c(rnorm(30, -3), rnorm(30,3))
tmp
hist(tmp)

x<-cbind(x=tmp, y=rev(tmp))
plot(x)
```

> Q. Can we use kmenas() to cluster this setting k to 2 and nstart to 20?

```{r}
km<- kmeans(x, centers=2, nstart = 20)
km
```

>. Q: How many points are in each cluster?

```{r}
km$size
```

>. Q: what 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```

>. Q: what 'component' of your result object details cluster center?

```{r}
km$centers
```

>. PLot x colored by the kmenas cluster assignment and add custers as blue points. 

```{r}
plot(x, col=km$cluster) 
points(km$centers, col= "blue", pch=15, cex=2)
```

# Hierchical clustering

A big limitation with k-means is that we have to tell it K (the number of clusters we want). 

Analyze this sme data with hclust()
Demonstrate the ise of dist(), hclust(), plot() and cutree(), functions to do clustering. Generate dendrograms and return cluster assignment/ membership vector...
```{r}
hc<- hclust(dist(x))
hc
```

There is a plot method for hclust result objects. Let's see it:

```{r}
plot(hc)
```

To get our cluster membership vector we have to do more work. We have to "cut" the tree where we think it makes sense. For this we want to use `cutree()` function. 

```{r}
cutree(hc, h=6)
```

You can also call cutree() setting k=the number of grps/clusters you want. 

```{r}
grps<- cutree(hc, k=2)
```


Make our results plot:

```{r}
plot(x, col=grps)
```




# Hands-on Lab: Principal Component Analysis

## Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
###A: There are 17 rows and 4 columns in my data frame, we can get this answer by using `dim()` or `nrow()` and `ncol()`.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```


```{r}
nrow(x)
ncol(x)
```

However, this should say we have 4 columns not five, let's fix it:
One way
```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```
This is dangerous, we can end up deleting a country's data.

Another way:
```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names=1)
head(x)
```

```{r}
dim(x)
```
This is what we want. 

#Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?
### A: We should prefer the second way to do it, by setting the row name to one. This is better because we could end up deleting impoartant data is we just override x with x-1 like in the first way. 



#Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), col=rainbow(17))
```


```{r}
barplot(as.matrix(x), col=rainbow(17), beside = T)
```

#Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The resulting figure seems to show an expected trend line as a result of the data that we have available. If a given point lies on the diagnoal, then it is foloowing the general trend that the data has given us. If a given point is further away from diagonal, then it is not following the expected trend from the data. 

```{r}
pairs(x, col=rainbow(17), pch=16)
```
#Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?
From my understanding of the data, it seems that England and North Ireland seem to have the most dramatic differences, since their data plots are further apart.  

# PCA to the rescue!

The main fucntion in base R for PCA is `prompt()` 
This wants the transpose of our data. 

```{r}
pca<- prcomp(t(x))
summary(pca)
```

```{r}
attributes(pca)
```


#Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.
```{r}
plot(pca$x[,1], pca$x[,2])

```

```{r}

# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```

# Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
colors<- c("orange", "red", "blue", "green")
text(pca$x[,1], pca$x[,2], colnames(x), col=colors)
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

#Q9: Generate a similar ‘loadings plot’ for PC2. What two food groups feature prominantely and what does PC2 maninly tell us about?

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

The two food groups most prominent in this plot are fresh potatoes and soft drinks. PC2 tells us something a little different than PC1. In PC2, fresh potatoes in N. Ireland to right, soft drinks push it to the left.
Overall PC2 accounts for less variance so there's less variacne (29%) shown in the distribution (loading scores  are closer to zero than in pc1). 


#Biplot
## The inbuilt biplot() can be useful for small datasets 

```{r}
biplot(pca)
```

#Q10: How many genes and samples are in this data set?
```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
dim(rna.data)
```
There are 100 genes in this data set. 



## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")

```{r}
## Again we have to take the transpose of our data 
pca <- prcomp(t(rna.data), scale=TRUE)
 
## Simple un polished plot of pc1 and pc2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca)
```

```{r}
plot(pca, main="Quick scree plot")
```

```{r}
## Variance captured per PC 
pca.var <- pca$sdev^2

## Percent variance is often more informative to look at 
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)

# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()
```


```{r}
# Add a 'wt' and 'ko' "condition" column
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE)
p
```

```{r}
p + labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clealy seperates wild-type from knock-out samples",
       x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="BIMM143 example data") +
     theme_bw()
```


#Gene loadings

```{r}
loading_scores <- pca$rotation[,1]

## Find the top 10 measurements (genes) that contribute
## most to PC1 in either direction (+ or -)
gene_scores <- abs(loading_scores) 
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)

## show the names of the top 10 genes
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes 
```




